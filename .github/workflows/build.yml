name: Build and Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      version:
        description: 'Release version (e.g., v1.0.0)'
        required: true
        default: 'v1.0.0'
      llamacpp_version:
        description: 'llama.cpp version to download (e.g., b3259)'
        required: false
        default: 'latest'
      windows_binary_url:
        description: 'Custom Windows binary URL (optional)'
        required: false
      macos_binary_url:
        description: 'Custom macOS binary URL (optional)'
        required: false
      linux_binary_url:
        description: 'Custom Linux binary URL (optional)'
        required: false

env:
  LLAMACPP_VERSION: ${{ github.event.inputs.llamacpp_version || 'b5997' }}
  WINDOWS_BINARY_URL: ${{ github.event.inputs.windows_binary_url || '' }}
  MACOS_BINARY_URL: ${{ github.event.inputs.macos_binary_url || '' }}
  LINUX_BINARY_URL: ${{ github.event.inputs.linux_binary_url || '' }}

jobs:
  build:
    runs-on: ${{ matrix.os }}
    
    strategy:
      matrix:
        os: [windows-latest, macos-latest, ubuntu-latest]
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Download llama.cpp binaries
      run: |
        mkdir -p bin/windows bin/macos bin/linux
        
        # è¨­å®šä¸‹è¼‰ URLï¼ˆå¯ä»¥åœ¨é€™è£¡ä¿®æ”¹ç‰ˆæœ¬æˆ–æª”æ¡ˆåç¨±ï¼‰
        LLAMACPP_VERSION="master"  # å¯æ”¹ç‚ºç‰¹å®šç‰ˆæœ¬å¦‚ "b3259"
        
        if [ "${{ matrix.os }}" = "windows-latest" ]; then
          # Windows CUDA ç‰ˆæœ¬
          DOWNLOAD_URL="https://github.com/ggml-org/llama.cpp/releases/latest/download/llama-${LLAMACPP_VERSION}-bin-win-cuda-cu12.2.0-x64.zip"
          EXTRACT_DIR="llama-${LLAMACPP_VERSION}-bin-win-cuda-cu12.2.0-x64"
          
          echo "Downloading Windows binaries from: $DOWNLOAD_URL"
          curl -L -o llama-cpp.zip "$DOWNLOAD_URL"
          
          if [ $? -eq 0 ]; then
            unzip -q llama-cpp.zip
            if [ -d "$EXTRACT_DIR" ]; then
              cp "$EXTRACT_DIR/rpc-server.exe" bin/windows/ 2>/dev/null || echo "Warning: rpc-server.exe not found"
              cp "$EXTRACT_DIR/llama-server.exe" bin/windows/ 2>/dev/null || echo "Warning: llama-server.exe not found"
              cp "$EXTRACT_DIR"/*.dll bin/windows/ 2>/dev/null || echo "Warning: DLL files not found"
              echo "Windows binaries copied successfully"
            else
              echo "Error: Extract directory $EXTRACT_DIR not found"
              ls -la
            fi
          else
            echo "Error: Failed to download Windows binaries"
            exit 1
          fi
          
        elif [ "${{ matrix.os }}" = "macos-latest" ]; then
          # macOS ARM64 ç‰ˆæœ¬
          DOWNLOAD_URL="https://github.com/ggml-org/llama.cpp/releases/latest/download/llama-${LLAMACPP_VERSION}-bin-macos-arm64.zip"
          EXTRACT_DIR="llama-${LLAMACPP_VERSION}-bin-macos-arm64"
          
          echo "Downloading macOS binaries from: $DOWNLOAD_URL"
          curl -L -o llama-cpp.zip "$DOWNLOAD_URL"
          
          if [ $? -eq 0 ]; then
            unzip -q llama-cpp.zip
            if [ -d "$EXTRACT_DIR" ]; then
              cp "$EXTRACT_DIR/rpc-server" bin/macos/ 2>/dev/null || echo "Warning: rpc-server not found"
              cp "$EXTRACT_DIR/llama-server" bin/macos/ 2>/dev/null || echo "Warning: llama-server not found"
              chmod +x bin/macos/* 2>/dev/null
              echo "macOS binaries copied successfully"
            else
              echo "Error: Extract directory $EXTRACT_DIR not found"
              ls -la
            fi
          else
            echo "Error: Failed to download macOS binaries"
            exit 1
          fi
          
        else
          # Linux x64 ç‰ˆæœ¬
          DOWNLOAD_URL="https://github.com/ggml-org/llama.cpp/releases/latest/download/llama-${LLAMACPP_VERSION}-bin-ubuntu-vulkan-x64.zip"
          EXTRACT_DIR="llama-${LLAMACPP_VERSION}-bin-ubuntu-x64"
          
          echo "Downloading Linux binaries from: $DOWNLOAD_URL"
          curl -L -o llama-cpp.zip "$DOWNLOAD_URL"
          
          if [ $? -eq 0 ]; then
            unzip -q llama-cpp.zip
            if [ -d "$EXTRACT_DIR" ]; then
              cp "$EXTRACT_DIR/rpc-server" bin/linux/ 2>/dev/null || echo "Warning: rpc-server not found"
              cp "$EXTRACT_DIR/llama-server" bin/linux/ 2>/dev/null || echo "Warning: llama-server not found"
              chmod +x bin/linux/* 2>/dev/null
              echo "Linux binaries copied successfully"
            else
              echo "Error: Extract directory $EXTRACT_DIR not found"
              ls -la
            fi
          else
            echo "Error: Failed to download Linux binaries"
            exit 1
          fi
        fi
        
        # é©—è­‰æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        echo "Verifying downloaded binaries:"
        ls -la bin/*/
      shell: bash
      
    - name: Build Electron app
      run: npm run build
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-${{ matrix.os }}
        path: dist/
        retention-days: 7

  release:
    needs: build
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/') || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: dist-all/
        
    - name: Prepare release assets
      run: |
        mkdir -p release-assets
        
        # Windows
        if [ -d "dist-all/dist-windows-latest" ]; then
          cd dist-all/dist-windows-latest
          if [ -f *.exe ]; then
            cp *.exe ../../release-assets/distributed-llm-inference-windows.exe
          fi
          if [ -d "win-unpacked" ]; then
            cd win-unpacked
            zip -r ../../../release-assets/distributed-llm-inference-windows.zip .
            cd ..
          fi
          cd ../..
        fi
        
        # macOS
        if [ -d "dist-all/dist-macos-latest" ]; then
          cd dist-all/dist-macos-latest
          if [ -f *.dmg ]; then
            cp *.dmg ../../release-assets/distributed-llm-inference-macos.dmg
          fi
          if [ -d "mac" ]; then
            cd mac
            zip -r ../../../release-assets/distributed-llm-inference-macos.zip .
            cd ..
          fi
          cd ../..
        fi
        
        # Linux
        if [ -d "dist-all/dist-ubuntu-latest" ]; then
          cd dist-all/dist-ubuntu-latest
          if [ -f *.AppImage ]; then
            cp *.AppImage ../../release-assets/distributed-llm-inference-linux.AppImage
          fi
          if [ -d "linux-unpacked" ]; then
            cd linux-unpacked
            tar -czf ../../../release-assets/distributed-llm-inference-linux.tar.gz .
            cd ..
          fi
          cd ../..
        fi
        
    - name: Get version
      id: version
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "version=${{ github.event.inputs.version }}" >> $GITHUB_OUTPUT
        else
          echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
        fi
        
    - name: Create Release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: ${{ steps.version.outputs.version }}
        name: Release ${{ steps.version.outputs.version }}
        draft: true
        prerelease: false
        body: |
          ## ğŸš€ åˆ†ä½ˆå¼ LLM æ¨ç†æ¡Œé¢æ‡‰ç”¨ç¨‹å¼ ${{ steps.version.outputs.version }}
          
          ### ğŸ“¦ ä¸‹è¼‰èªªæ˜
          
          - **Windows**: `distributed-llm-inference-windows.exe` (å®‰è£ç¨‹å¼) æˆ– `distributed-llm-inference-windows.zip` (å…å®‰è£ç‰ˆ)
          - **macOS**: `distributed-llm-inference-macos.dmg` (å®‰è£ç¨‹å¼) æˆ– `distributed-llm-inference-macos.zip` (å…å®‰è£ç‰ˆ)
          - **Linux**: `distributed-llm-inference-linux.AppImage` (å¯åŸ·è¡Œæª”) æˆ– `distributed-llm-inference-linux.tar.gz` (å…å®‰è£ç‰ˆ)
          
          ### âš ï¸ é‡è¦æé†’
          
          1. **å®‰å…¨è­¦å‘Š**: è«‹åƒ…åœ¨å¯ä¿¡çš„å…§ç¶²ç’°å¢ƒä¸­ä½¿ç”¨
          2. **æ¨¡å‹æª”æ¡ˆ**: éœ€è¦è‡ªè¡Œä¸‹è¼‰ `.gguf` æ ¼å¼çš„æ¨¡å‹æª”æ¡ˆ
          3. **ç¶²è·¯è¨­å®š**: ç¢ºä¿é˜²ç«ç‰†é–‹æ”¾ç«¯å£ 50052 å’Œ 8080
          
          ### ğŸ†• ä¸»è¦åŠŸèƒ½
          
          - ğŸ–¥ï¸ ç¾ä»£åŒ–æ·±è‰²/æ˜äº®æ¨¡å¼ä»‹é¢
          - ğŸŒ è‡ªå‹•ç¯€é»ç™¼ç¾ + æ‰‹å‹•æ·»åŠ 
          - âš¡ é«˜æ•ˆåˆ†ä½ˆå¼æ¨ç†
          - ğŸ“ è‡ªå®šç¾©æ¨¡å‹è·¯å¾‘ç®¡ç†
          - ğŸ”§ ä¸¦è¡Œè«‹æ±‚æ•¸æ§åˆ¶
          - ğŸ“Š å¯¦æ™‚æ—¥èªŒç›£æ§
          - ğŸ” ç¯€é»é€£æ¥æª¢æŸ¥
          
          ### ğŸ“‹ ä½¿ç”¨æ­¥é©Ÿ
          
          1. ä¸‹è¼‰å°æ‡‰å¹³å°çš„æª”æ¡ˆ
          2. è§£å£“ç¸®æˆ–å®‰è£
          3. å°‡ `.gguf` æ¨¡å‹æª”æ¡ˆæ”¾å…¥ `models/` è³‡æ–™å¤¾
          4. åŸ·è¡Œç¨‹å¼ä¸¦é–‹å§‹ä½¿ç”¨
          
          ---
          
          **å®Œæ•´èªªæ˜è«‹åƒé–±**: [README.md](https://github.com/${{ github.repository }}/blob/main/README.md)
        files: |
          release-assets/*
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}